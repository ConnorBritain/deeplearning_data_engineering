# Data Ingestion and DataOps üì•‚öôÔ∏è

This repository contains materials for **Course 2: Data Ingestion and DataOps** of the DeepLearning.AI Data Engineering Professional Certificate. The course focuses on data ingestion techniques and DataOps practices to build efficient and reliable data pipelines.

## üìñ Overview

We delve into various data ingestion methods, working with different source systems, and the principles of DataOps. The course emphasizes automation, orchestration, and monitoring to enhance data pipeline efficiency and reliability.

## üóì Course Structure

### **Week 1: Working with Source Systems**

- **Learning Objectives:**
  1. Identify different data formats and appropriate source systems.
  2. Explain the difference between relational and NoSQL databases.
  3. Understand ACID compliance principles.
  4. Apply CRUD operations on relational and NoSQL databases.
  5. Interact with object storage.
  6. Differentiate between message queues and streaming platforms.
  7. Explain the basics of cloud networking.
  8. Troubleshoot database connection errors.

### **Week 2: Data Ingestion**

- **Learning Objectives:**
  1. Explain batch vs. streaming ingestion and their use cases.
  2. Identify ways to ingest data from various source systems.
  3. Differentiate between ETL and ELT ingestion patterns.
  4. Interact with REST APIs for data ingestion.
  5. Understand components of event-streaming platforms.
  6. Use event streaming platforms for data ingestion.

### **Week 3: DataOps**

- **Learning Objectives:**
  1. Explain how DevOps concepts like CI/CD apply to DataOps.
  2. Define Infrastructure as Code (IaC).
  3. Use Terraform to provision AWS resources.
  4. Differentiate between DevOps and DataOps observability.
  5. Apply data quality tests using Great Expectations.
  6. Monitor relevant data quality metrics.

### **Week 4: Orchestration, Monitoring, and Automating Your Data Pipelines**

- **Learning Objectives:**
  1. Explain the benefits of orchestration in data pipelines.
  2. Use core components of Apache Airflow.
  3. Build data pipelines with Airflow DAGs.
  4. Apply best practices in building Airflow DAGs.
  5. Integrate data quality testing in Airflow pipelines.

## üõ† Skills Developed

- Data ingestion techniques (batch and streaming).
- Working with relational and NoSQL databases.
- Implementing DataOps practices.
- Infrastructure as Code using Terraform.
- Orchestrating pipelines with Apache Airflow.
- Data quality assurance with Great Expectations.

## üîß Technologies & Tools

- **Programming Languages:** Python, SQL
- **Data Ingestion:** REST APIs, Message Queues, Streaming Platforms
- **Databases:** MySQL, PostgreSQL, MongoDB
- **Cloud Platform:** AWS
- **Infrastructure as Code:** Terraform
- **Orchestration:** Apache Airflow
- **Data Quality:** Great Expectations

## üìÇ Repository Contents

- **/graded_assessments/**: Solutions to graded assessments.
- **/labs/**: Lab exercises and solutions.
- **/scripts/**: Data ingestion scripts for APIs and streaming platforms.
- **/terraform/**: Terraform configurations for AWS resource provisioning.
- **/airflow_dags/**: Airflow DAGs for pipeline orchestration.
- **/data_quality_tests/**: Great Expectations tests and configurations.
- **/notes/**: Personal notes on DataOps and orchestration best practices.
- **/quizzes/**: Quiz solutions with explanations.

## üìö Resources

- **Official Course Link:** [DeepLearning.AI Data Engineering Professional Certificate](https://deeplearning.ai/courses/data-engineering)

## üì´ Contact

Connect with me on [LinkedIn](https://www.linkedin.com/in/connorengland) or reach out via [email](mailto:connor.r.england@gmail.com) for collaborations or questions!

---
